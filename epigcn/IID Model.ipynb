{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data wrangling approach\n",
    "\n",
    "Things to verify:\n",
    "1. Are the 4 features(counts) useful as predictors or is it okay to throw them away?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling Approach\n",
    "\n",
    "1. Generate sliding windowed data set as before\n",
    "2. Train test split as before\n",
    "3. Unroll matrix at country of origin dimension and treat each sample differently\n",
    "4. Standardize and log transform on just 1 feature\n",
    "5. Model the spatio-temporal data on different baselines (minus graph data)\n",
    "\n",
    "# Visualization approach\n",
    "\n",
    "1. Model the diffusion on an actual world map?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "import h5py\n",
    "import gzip\n",
    "import json\n",
    "import os\n",
    "import tqdm\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interact_manual\n",
    "\n",
    "logging.basicConfig()\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.WARN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "incidences = None\n",
    "with h5py.File(\"data/EpiGCN/processing/incidences.hdf5\", \"r\") as f:\n",
    "    incidences = f['incidences'][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3181, 232, 366, 4)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "incidences.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize by population to get fraction of population infected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "countries_metadata = json.load(open('data/countries_metadata.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open('data/output_0000.h5.gz') as fp:\n",
    "    f = h5py.File(fp,'r')\n",
    "    compartments = f['dset'].attrs['Columns']\n",
    "    country_ids = f['dset'].attrs['SelectedCountries']\n",
    "    prevalence = np.array(f['dset/countries/cumulative'], dtype=np.int64)\n",
    "    incidence = np.array(f['dset/countries/transitions'], dtype=np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 71, 140, 149, 222, 103, 183, 199,  53, 113,  29, 221, 207,  47,\n",
       "        28, 213,  76, 157, 109, 181, 154, 164, 178,  50,  79, 108, 185,\n",
       "         7,  31, 128,  91,  75, 227,  70,  80,  81,  12, 179, 176, 172,\n",
       "       210,  94,  67, 200, 129, 218, 215, 110, 188,   2,  82, 187, 150,\n",
       "        61, 177, 119,  16, 180,  95, 208, 147,  54,  86,  15,  88,  60,\n",
       "        90, 158, 230,  58,   8, 144,  89, 202, 130, 104, 138,  10,  13,\n",
       "       161,  35, 193, 111, 229,  72,  48, 153,   5,  84, 117,  22, 191,\n",
       "       112,  73, 120, 214, 107,  62, 116,  85,   0, 206, 115,  34,  25,\n",
       "        45, 160, 205,  49,  78, 159,  63, 106, 201, 125, 216, 122, 134,\n",
       "         3,  52, 223,  43, 123,  41,  51, 135, 169, 101, 197,  39, 139,\n",
       "         1,  27,  55,  83,  23,   6,  65, 190,  74, 231, 145, 196, 211,\n",
       "       141,  32, 121, 155, 137, 114,  19,  99, 146,  11, 136, 220, 195,\n",
       "       105, 170, 219,  98, 203, 217, 173,  64, 228, 127, 212,  42, 156,\n",
       "       192, 182,  46,  59,  18, 102,  36, 168,  26,  87, 126, 124,  40,\n",
       "        97,  66,  92, 209, 174,  93, 171, 224, 226, 163,  20, 225, 152,\n",
       "        30,  38,  44, 204, 131, 194, 132,  68, 162, 175, 198, 100,  37,\n",
       "       143,   9,  96, 165, 167, 166, 151,  69, 133, 142, 186,  21, 189,\n",
       "         4, 184,  17, 118,  14,  24, 148,  33,  77,  56,  57])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "country_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_countries_population(country_ids, countries_metadata):\n",
    "    \"\"\"\n",
    "    countries are ordered using country ids, we query for their population and return it accordingly.\n",
    "    \"\"\"\n",
    "    return np.array(\n",
    "        list(map(lambda cid: countries_metadata[str(cid)][\"population\"], country_ids))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries_population = get_countries_population(country_ids, countries_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "incidences = (incidences / countries_population[np.newaxis, :, np.newaxis, np.newaxis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min 0.0\n",
      "max 0.04334470989761092\n",
      "mean 0.0009119511804321677\n",
      "std 0.0023550459485025665\n"
     ]
    }
   ],
   "source": [
    "print(\"min\", incidences.min())\n",
    "print(\"max\", incidences.max())\n",
    "print(\"mean\", incidences.mean())\n",
    "print(\"std\", incidences.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that incidences is (hopefully) normalized by population, we can use sliding windows and create train test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File('data/EpiGCN/processing/incidences_normalized.hdf5', 'w') as f:\n",
    "    f.create_dataset('incidences', data=incidences, compression=\"lzf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load incidences normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "incidences_normalized = None\n",
    "with h5py.File(\"data/EpiGCN/processing/incidences_normalized.hdf5\", \"r\") as f:\n",
    "    incidences_normalized = f['incidences'][:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create sliding window dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3181, 232, 366, 4)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "incidences_normalized.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "def generate_sliding_windows_dataset(incidences, t_lag=10, window_size=10):\n",
    "    \"\"\"\n",
    "    :t_lag: \n",
    "    :window_size: assumes this is less than 366\n",
    "    \"\"\"\n",
    "    with h5py.File(\"data/EpiGCN/dataset.hdf5\", \"w\") as fw:\n",
    "        x_ts = fw.create_dataset(\n",
    "            \"x_ts\",\n",
    "            compression=\"lzf\",\n",
    "            shape=((366 - t_lag - window_size + 1) * 3181, 232, window_size, 4),\n",
    "            maxshape=(None, 232, window_size, 4),\n",
    "        )\n",
    "        y_ts = fw.create_dataset(\n",
    "            \"y_ts\",\n",
    "            compression=\"lzf\",\n",
    "            shape=((366 - t_lag - window_size + 1) * 3181, 232, 1, 4),\n",
    "            maxshape=(None, 232, 1, 4),\n",
    "        )\n",
    "\n",
    "        start, end = t_lag + window_size, 367\n",
    "        window_indices = np.arange(start, end, step=1)\n",
    "        target_ix = 0\n",
    "        for ix in list(reversed(window_indices)):\n",
    "            print(ix, target_ix)\n",
    "            \n",
    "            x = incidences[:, :, (ix - window_size) : ix, :]\n",
    "            y = incidences[:, :, (ix - window_size - t_lag), :]\n",
    "\n",
    "            ## expand dims for y so that there is a single dimension for y\n",
    "            ## and then swap last 2 axis so that counts are again the last axis\n",
    "            y = np.swapaxes(np.expand_dims(y, -1), -1, -2)\n",
    "            x_ts[target_ix * 3181: (target_ix + 1) * 3181] = x\n",
    "            y_ts[target_ix * 3181: (target_ix + 1) * 3181] = y\n",
    "            target_ix += 1\n",
    "            clear_output(wait=True)\n",
    "        #     return np.stack(x_data), np.stack(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 346\n"
     ]
    }
   ],
   "source": [
    "generate_sliding_windows_dataset(incidences_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1103807, 232, 10, 4)\n",
      "(1103807, 232, 1, 4)\n"
     ]
    }
   ],
   "source": [
    "with h5py.File(\"data/EpiGCN/dataset.hdf5\", \"r\") as f:\n",
    "    print(f['x_ts'].shape)\n",
    "    print(f['y_ts'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def get_train_test_indices(t_lag=10, window_size=10, test_ratio=0.1, random_seed=42):\n",
    "    num_samples = (366 - t_lag - window_size + 1) * 3181\n",
    "    train_indices, test_indices = train_test_split(\n",
    "        range(num_samples),\n",
    "        test_size=int(num_samples * test_ratio),\n",
    "        random_state=random_seed,\n",
    "        shuffle=True,\n",
    "    )\n",
    "    return train_indices, test_indices\n",
    "\n",
    "\n",
    "train_indices, test_indices = get_train_test_indices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ts = None\n",
    "y_ts = None\n",
    "\n",
    "with h5py.File(\"data/EpiGCN/dataset.hdf5\", \"r\") as f:\n",
    "    x_ts = f[\"x_ts\"][:]\n",
    "    y_ts = f[\"y_ts\"][:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This loads the entire dataset in memory - approx 50 GB and splits into train test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_train_test_in_mem(train_indices, test_indices):\n",
    "    x_train = x_ts[train_indices]\n",
    "    y_train = y_ts[train_indices]\n",
    "    x_test = x_ts[test_indices]\n",
    "    y_test = y_ts[test_indices]\n",
    "    \n",
    "    print(\"finished splitting\")\n",
    "    with h5py.File('data/EpiGCN/processing/train_data.hdf5', 'w') as f:\n",
    "        f.create_dataset('x_train', data=x_train, compression=\"lzf\")\n",
    "        f.create_dataset('y_train', data=y_train, compression=\"lzf\")\n",
    "        \n",
    "    with h5py.File('data/EpiGCN/processing/test_data.hdf5', 'w') as f:\n",
    "        f.create_dataset('x_test', data=x_test, compression=\"lzf\")\n",
    "        f.create_dataset('y_test', data=y_test, compression=\"lzf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished splitting\n"
     ]
    }
   ],
   "source": [
    "split_into_train_test_in_mem(train_indices, test_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method allows streaming train test split of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_train_test(train_incides, test_indices, window_size=10):\n",
    "    with h5py.File(\"data/EpiGCN/dataset.hdf5\", \"r\") as f:\n",
    "        with h5py.File(\"data/EpiGCN/train_test.hdf5\", \"w\") as fw:\n",
    "            x_ts = f[\"x_ts\"][:]\n",
    "            print(\"x data loaded\")\n",
    "\n",
    "            y_ts = f[\"y_ts\"][:]\n",
    "\n",
    "            print(\"y data loaded\")\n",
    "\n",
    "            x_train = fw.create_dataset(\n",
    "                \"x_train\",\n",
    "                compression=\"lzf\",\n",
    "                shape=(len(train_indices), 232, window_size, 4),\n",
    "                maxshape=(None, 232, window_size, 4),\n",
    "            )\n",
    "            y_train = fw.create_dataset(\n",
    "                \"y_train\",\n",
    "                compression=\"lzf\",\n",
    "                shape=(len(train_indices), 232, 1, 4),\n",
    "                maxshape=(None, 232, 1, 4),\n",
    "            )\n",
    "            x_test = fw.create_dataset(\n",
    "                \"x_test\",\n",
    "                compression=\"lzf\",\n",
    "                shape=(len(test_indices), 232, window_size, 4),\n",
    "                maxshape=(None, 232, window_size, 4),\n",
    "            )\n",
    "            y_test = fw.create_dataset(\n",
    "                \"y_test\",\n",
    "                compression=\"lzf\",\n",
    "                shape=(len(test_indices), 232, 1, 4),\n",
    "                maxshape=(None, 232, 1, 4),\n",
    "            )\n",
    "\n",
    "            train_current_ix, test_current_ix = 0, 0\n",
    "            batch_size = 5000\n",
    "            for ix in range(0, len(train_indices), batch_size):\n",
    "                print(\n",
    "                    \"Training progress: {:.2f}\".format(\n",
    "                        (train_current_ix * batch_size * 1.0) / len(train_incides)\n",
    "                    )\n",
    "                )\n",
    "                ixes = sorted(train_indices[ix : ix + batch_size])\n",
    "                x_train[ix : ix + batch_size] = x_ts[ixes]\n",
    "                y_train[ix : ix + batch_size] = y_ts[ixes]\n",
    "                train_current_ix += 1\n",
    "                clear_output(wait=True)\n",
    "\n",
    "            for test_ix in tqdm.tqdm_notebook(test_indices):\n",
    "                print(\n",
    "                    \"Testing progress: {:.2f}\".format(\n",
    "                        (test_current_ix * 1.0) / len(test_indices)\n",
    "                    )\n",
    "                )\n",
    "                x_test[test_current_ix] = x_ts[test_ix]\n",
    "                y_test[test_current_ix] = y_ts[test_ix]\n",
    "                test_current_ix += 1\n",
    "                clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split_into_train_test(train_indices, test_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = None\n",
    "y_train = None\n",
    "x_test = None\n",
    "y_test = None\n",
    "\n",
    "with h5py.File('data/EpiGCN/processing/train_data.hdf5', 'r') as f:\n",
    "    x_train = f['x_train'][:] \n",
    "    y_train = f['y_train'][:]\n",
    "\n",
    "with h5py.File('data/EpiGCN/processing/test_data.hdf5', 'r') as f:\n",
    "    x_test = f['x_test'][:] \n",
    "    y_test = f['y_test'][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y test size (110380, 232, 1, 4)\n"
     ]
    }
   ],
   "source": [
    "print(\"y test size\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:wandb.run_manager:shutting down system stats and metadata service\n",
      "INFO:wandb.run_manager:stopping streaming files and file change observer\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "\n",
    "train_dataset = TensorDataset(torch.FloatTensor(x_train), torch.FloatTensor(y_train))\n",
    "test_dataset = TensorDataset(torch.FloatTensor(x_test), torch.FloatTensor(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params = {\"batch_size\": 12, \"shuffle\": True, \"num_workers\": 4}\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=512, shuffle=True, num_workers=12)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=512, shuffle=False, num_workers=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaselineLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers=1):\n",
    "        super(BaselineLSTM, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            batch_first=True,\n",
    "            num_layers=num_layers,\n",
    "        )\n",
    "        self.linear = nn.Linear(hidden_size * num_layers, input_size)\n",
    "\n",
    "    def forward(self, lstm_input):\n",
    "        b, n_countries, seq_len, n_compartments = lstm_input.size()\n",
    "        lstm_input = lstm_input.permute(0, 2, 1, 3)\n",
    "        _, (hn, _) = self.lstm(lstm_input.contiguous().view(b, seq_len, -1))\n",
    "        hn = hn.permute(1, 0, 2)\n",
    "        return self.linear(hn.reshape(b, -1)).view(b, n_countries, 1, n_compartments)\n",
    "\n",
    "\n",
    "class BaselineGRU(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers=1):\n",
    "        super(BaselineGRU, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.GRU(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            batch_first=True,\n",
    "            num_layers=num_layers,\n",
    "        )\n",
    "        self.linear = nn.Linear(hidden_size * num_layers, input_size)\n",
    "\n",
    "    def forward(self, lstm_input):\n",
    "        b, n_countries, seq_len, n_compartments = lstm_input.size()\n",
    "        lstm_input = lstm_input.permute(0, 2, 1, 3)\n",
    "        _, hn = self.lstm(lstm_input.contiguous().view(b, seq_len, -1))\n",
    "        hn = hn.permute(1, 0, 2)\n",
    "        return self.linear(hn.reshape(b, -1)).view(b, n_countries, 1, n_compartments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from ignite.engine import Events, create_supervised_trainer, create_supervised_evaluator\n",
    "from ignite.handlers import ModelCheckpoint\n",
    "from ignite.metrics import Loss\n",
    "from ignite.contrib.handlers.tensorboard_logger import *\n",
    "import wandb\n",
    "\n",
    "\n",
    "class ModelTrainer:\n",
    "    def __init__(self, run_name, **training_params):\n",
    "        self.run_name = run_name\n",
    "        self.training_params = training_params\n",
    "\n",
    "        params_str = \",\".join(\n",
    "            [\"{}={}\".format(k, v) for k, v in training_params.items()]\n",
    "        )\n",
    "        wandb.init(sync_tensorboard=True, name=self.run_name)\n",
    "        wandb.config.initial_lr = training_params[\"lr\"]\n",
    "        wandb.config.input_size = training_params[\"input_size\"]\n",
    "        wandb.config.hidden_size = training_params[\"hidden_size\"]\n",
    "        wandb.config.num_layers = training_params[\"num_layers\"]\n",
    "        wandb.config.param_str = params_str\n",
    "\n",
    "    def train(self, model_class, max_epochs=100):\n",
    "        model = model_class(\n",
    "            input_size=self.training_params[\"input_size\"],\n",
    "            hidden_size=self.training_params[\"hidden_size\"],\n",
    "            num_layers=self.training_params[\"num_layers\"],\n",
    "        )\n",
    "\n",
    "        wandb.watch(model)\n",
    "\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=params[\"lr\"])\n",
    "        lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, patience=3, factor=0.5\n",
    "        )\n",
    "\n",
    "        mse = torch.nn.MSELoss()\n",
    "        mae = torch.nn.L1Loss()\n",
    "\n",
    "        tb_logger = TensorboardLogger(\n",
    "            log_dir=\"experiments/tb_logs/runs/IID_all_compartments_{}\".format(\n",
    "                params_str\n",
    "            )\n",
    "        )\n",
    "\n",
    "        trainer = create_supervised_trainer(model, optimizer, mae, device=device)\n",
    "\n",
    "        metrics = {\"mae\": Loss(mae), \"mse\": Loss(mse)}\n",
    "\n",
    "        train_evaluator = create_supervised_evaluator(\n",
    "            model, metrics=metrics, device=device\n",
    "        )\n",
    "        validation_evaluator = create_supervised_evaluator(\n",
    "            model, metrics=metrics, device=device\n",
    "        )\n",
    "\n",
    "        def score_function(trainer):\n",
    "            validation_evaluator.run(test_dataloader)\n",
    "            metrics = validation_evaluator.state.metrics\n",
    "            mae = metrics[\"mae\"]\n",
    "            return -mae\n",
    "\n",
    "        checkpointer = ModelCheckpoint(\n",
    "            \"model_checkpoints\",\n",
    "            \"IID_all_compartments-\" + params_str,\n",
    "            score_function=score_function,\n",
    "            score_name=\"mae\",\n",
    "            n_saved=2,\n",
    "            create_dir=True,\n",
    "            save_as_state_dict=True,\n",
    "            require_empty=False,\n",
    "        )\n",
    "\n",
    "        trainer.add_event_handler(Events.EPOCH_COMPLETED, checkpointer, {\"epi\": model})\n",
    "\n",
    "        tb_logger.attach(\n",
    "            trainer,\n",
    "            log_handler=OutputHandler(\n",
    "                tag=\"training\", output_transform=lambda loss: {\"loss\": loss}\n",
    "            ),\n",
    "            event_name=Events.ITERATION_COMPLETED,\n",
    "        )\n",
    "\n",
    "        tb_logger.attach(\n",
    "            train_evaluator,\n",
    "            log_handler=OutputHandler(\n",
    "                tag=\"training\",\n",
    "                metric_names=[\"mae\", \"mse\"],\n",
    "                global_step_transform=global_step_from_engine(trainer),\n",
    "            ),\n",
    "            event_name=Events.EPOCH_COMPLETED,\n",
    "        )\n",
    "\n",
    "        tb_logger.attach(\n",
    "            validation_evaluator,\n",
    "            log_handler=OutputHandler(\n",
    "                tag=\"validation\",\n",
    "                metric_names=[\"mae\", \"mse\"],\n",
    "                global_step_transform=global_step_from_engine(trainer),\n",
    "            ),\n",
    "            event_name=Events.EPOCH_COMPLETED,\n",
    "        )\n",
    "\n",
    "        # Attach the logger to the trainer to log optimizer's parameters, e.g. learning rate at each iteration\n",
    "        tb_logger.attach(\n",
    "            trainer,\n",
    "            log_handler=OptimizerParamsHandler(optimizer),\n",
    "            event_name=Events.ITERATION_STARTED,\n",
    "        )\n",
    "\n",
    "        # Attach the logger to the trainer to log model's weights norm after each iteration\n",
    "        tb_logger.attach(\n",
    "            trainer,\n",
    "            log_handler=WeightsScalarHandler(model),\n",
    "            event_name=Events.ITERATION_COMPLETED,\n",
    "        )\n",
    "\n",
    "        # Attach the logger to the trainer to log model's weights as a histogram after each epoch\n",
    "        tb_logger.attach(\n",
    "            trainer,\n",
    "            log_handler=WeightsHistHandler(model),\n",
    "            event_name=Events.EPOCH_COMPLETED,\n",
    "        )\n",
    "\n",
    "        # Attach the logger to the trainer to log model's gradients norm after each iteration\n",
    "        tb_logger.attach(\n",
    "            trainer,\n",
    "            log_handler=GradsScalarHandler(model),\n",
    "            event_name=Events.ITERATION_COMPLETED,\n",
    "        )\n",
    "\n",
    "        # Attach the logger to the trainer to log model's gradients as a histogram after each epoch\n",
    "        tb_logger.attach(\n",
    "            trainer,\n",
    "            log_handler=GradsHistHandler(model),\n",
    "            event_name=Events.EPOCH_COMPLETED,\n",
    "        )\n",
    "\n",
    "        @trainer.on(Events.ITERATION_COMPLETED)\n",
    "        def log_training_loss(trainer):\n",
    "            logger.info(\n",
    "                \"Epoch[{}] Loss: {:.7f}\".format(\n",
    "                    trainer.state.epoch, trainer.state.output\n",
    "                )\n",
    "            )\n",
    "            wandb.log({\"train loss\": trainer.state.output})\n",
    "\n",
    "        @trainer.on(Events.EPOCH_COMPLETED)\n",
    "        def log_training_results(trainer):\n",
    "            train_evaluator.run(train_dataloader)\n",
    "            metrics = train_evaluator.state.metrics\n",
    "            logger.info(\n",
    "                \"Training Results - Epoch: {} MSE: {:.7f} MAE: {:.7f}\".format(\n",
    "                    trainer.state.epoch, metrics[\"mse\"], metrics[\"mae\"]\n",
    "                )\n",
    "            )\n",
    "\n",
    "        @trainer.on(Events.EPOCH_COMPLETED)\n",
    "        def log_validation_results(trainer):\n",
    "            validation_evaluator.run(test_dataloader)\n",
    "            metrics = validation_evaluator.state.metrics\n",
    "            logger.info(\n",
    "                \"Validation Results - Epoch: {} MSE: {:.7f} MAE: {:.7f}\".format(\n",
    "                    trainer.state.epoch, metrics[\"mse\"], metrics[\"mae\"]\n",
    "                )\n",
    "            )\n",
    "            lr_scheduler.step(metrics[\"mae\"])\n",
    "            wandb.log({\"val mae\": metrics[\"mae\"]})\n",
    "            wandb.log({\"val mse\": metrics[\"mse\"]})\n",
    "\n",
    "        trainer.run(train_dataloader, max_epochs=max_epochs)\n",
    "        tb_logger.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = ModelTrainer('IID_all_compartments_GRU', input_size=232*4, hidden_size=200, num_layers=1, lr=1e-4)\n",
    "trainer.train(BaselineGRU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/chnsh/epigcn\" target=\"_blank\">https://app.wandb.ai/chnsh/epigcn</a><br/>\n",
       "                Run page: <a href=\"https://app.wandb.ai/chnsh/epigcn/runs/0z8ijrdj\" target=\"_blank\">https://app.wandb.ai/chnsh/epigcn/runs/0z8ijrdj</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:wandb.jupyter:Failed to query for notebook name, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable\n",
      "wandb: WARNING Not logging key \"IID_all_compartments_lr=0.0001,input_size=928,hidden_size=200,num_layers=1/weights/lstm/weight_ih_l0\".  Histograms must have fewer than 512 bins\n",
      "wandb: WARNING Not logging key \"IID_all_compartments_lr=0.0001,input_size=928,hidden_size=200,num_layers=1/weights/lstm/weight_hh_l0\".  Histograms must have fewer than 512 bins\n",
      "wandb: WARNING Not logging key \"IID_all_compartments_lr=0.0001,input_size=928,hidden_size=200,num_layers=1/weights/lstm/bias_ih_l0\".  Histograms must have fewer than 512 bins\n",
      "wandb: WARNING Not logging key \"IID_all_compartments_lr=0.0001,input_size=928,hidden_size=200,num_layers=1/weights/lstm/bias_hh_l0\".  Histograms must have fewer than 512 bins\n",
      "wandb: WARNING Not logging key \"IID_all_compartments_lr=0.0001,input_size=928,hidden_size=200,num_layers=1/weights/linear/weight\".  Histograms must have fewer than 512 bins\n",
      "wandb: WARNING Not logging key \"IID_all_compartments_lr=0.0001,input_size=928,hidden_size=200,num_layers=1/weights/linear/bias\".  Histograms must have fewer than 512 bins\n"
     ]
    }
   ],
   "source": [
    "# from torch.utils.tensorboard import SummaryWriter\n",
    "# from ignite.engine import Events, create_supervised_trainer, create_supervised_evaluator\n",
    "# from ignite.handlers import ModelCheckpoint\n",
    "# from ignite.metrics import Loss\n",
    "# from ignite.contrib.handlers.tensorboard_logger import *\n",
    "# import wandb\n",
    "# wandb.init(sync_tensorboard=True, name='IID_all_compartments')\n",
    "\n",
    "# params = {\"lr\": 0.0001, \"input_size\": 232 * 4, \"hidden_size\": 200, \"num_layers\": 1}\n",
    "\n",
    "# params_str = \",\".join([\"{}={}\".format(k, v) for k, v in params.items()])\n",
    "\n",
    "# wandb.config.initial_lr = params['lr']\n",
    "# wandb.config.input_size = params['input_size']\n",
    "# wandb.config.hidden_size = params['hidden_size']\n",
    "# wandb.config.num_layers = params['num_layers']\n",
    "# wandb.config.param_str = params_str\n",
    "\n",
    "# model = BaselineLSTM(\n",
    "#     input_size=params[\"input_size\"],\n",
    "#     hidden_size=params[\"hidden_size\"],\n",
    "#     num_layers=params[\"num_layers\"],\n",
    "# )\n",
    "\n",
    "# wandb.watch(model)\n",
    "\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=params[\"lr\"])\n",
    "# lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=3, factor=0.5)\n",
    "\n",
    "# mse = torch.nn.MSELoss()\n",
    "# mae = torch.nn.L1Loss()\n",
    "\n",
    "# tb_logger = TensorboardLogger(\n",
    "#     log_dir=\"experiments/tb_logs/runs/IID_all_compartments_{}\".format(params_str)\n",
    "# )\n",
    "\n",
    "# trainer = create_supervised_trainer(model, optimizer, mae, device=device)\n",
    "\n",
    "# metrics = {\"mae\": Loss(mae), \"mse\": Loss(mse)}\n",
    "\n",
    "# train_evaluator = create_supervised_evaluator(model, metrics=metrics, device=device)\n",
    "# validation_evaluator = create_supervised_evaluator(\n",
    "#     model, metrics=metrics, device=device\n",
    "# )\n",
    "\n",
    "\n",
    "# def score_function(trainer):\n",
    "#     validation_evaluator.run(test_dataloader)\n",
    "#     metrics = validation_evaluator.state.metrics\n",
    "#     mae = metrics[\"mae\"]\n",
    "#     return -mae\n",
    "\n",
    "\n",
    "# checkpointer = ModelCheckpoint(\n",
    "#     \"model_checkpoints\",\n",
    "#     \"IID_all_compartments-\" + params_str,\n",
    "#     score_function=score_function,\n",
    "#     score_name=\"mae\",\n",
    "#     n_saved=2,\n",
    "#     create_dir=True,\n",
    "#     save_as_state_dict=True,\n",
    "#     require_empty=False,\n",
    "# )\n",
    "\n",
    "# trainer.add_event_handler(Events.EPOCH_COMPLETED, checkpointer, {\"epi\": model})\n",
    "\n",
    "# tb_logger.attach(\n",
    "#     trainer,\n",
    "#     log_handler=OutputHandler(\n",
    "#         tag=\"training\", output_transform=lambda loss: {\"loss\": loss}\n",
    "#     ),\n",
    "#     event_name=Events.ITERATION_COMPLETED,\n",
    "# )\n",
    "\n",
    "# tb_logger.attach(\n",
    "#     train_evaluator,\n",
    "#     log_handler=OutputHandler(\n",
    "#         tag=\"training\",\n",
    "#         metric_names=[\"mae\", \"mse\"],\n",
    "#         global_step_transform=global_step_from_engine(trainer),\n",
    "#     ),\n",
    "#     event_name=Events.EPOCH_COMPLETED,\n",
    "# )\n",
    "\n",
    "# tb_logger.attach(\n",
    "#     validation_evaluator,\n",
    "#     log_handler=OutputHandler(\n",
    "#         tag=\"validation\",\n",
    "#         metric_names=[\"mae\", \"mse\"],\n",
    "#         global_step_transform=global_step_from_engine(trainer),\n",
    "#     ),\n",
    "#     event_name=Events.EPOCH_COMPLETED,\n",
    "# )\n",
    "\n",
    "# # Attach the logger to the trainer to log optimizer's parameters, e.g. learning rate at each iteration\n",
    "# tb_logger.attach(\n",
    "#     trainer,\n",
    "#     log_handler=OptimizerParamsHandler(optimizer),\n",
    "#     event_name=Events.ITERATION_STARTED,\n",
    "# )\n",
    "\n",
    "# # Attach the logger to the trainer to log model's weights norm after each iteration\n",
    "# tb_logger.attach(\n",
    "#     trainer,\n",
    "#     log_handler=WeightsScalarHandler(model),\n",
    "#     event_name=Events.ITERATION_COMPLETED,\n",
    "# )\n",
    "\n",
    "# # Attach the logger to the trainer to log model's weights as a histogram after each epoch\n",
    "# tb_logger.attach(\n",
    "#     trainer, log_handler=WeightsHistHandler(model), event_name=Events.EPOCH_COMPLETED\n",
    "# )\n",
    "\n",
    "# # Attach the logger to the trainer to log model's gradients norm after each iteration\n",
    "# tb_logger.attach(\n",
    "#     trainer,\n",
    "#     log_handler=GradsScalarHandler(model),\n",
    "#     event_name=Events.ITERATION_COMPLETED,\n",
    "# )\n",
    "\n",
    "# # Attach the logger to the trainer to log model's gradients as a histogram after each epoch\n",
    "# tb_logger.attach(\n",
    "#     trainer, log_handler=GradsHistHandler(model), event_name=Events.EPOCH_COMPLETED\n",
    "# )\n",
    "\n",
    "\n",
    "# @trainer.on(Events.ITERATION_COMPLETED)\n",
    "# def log_training_loss(trainer):\n",
    "#     logger.info(\n",
    "#         \"Epoch[{}] Loss: {:.7f}\".format(trainer.state.epoch, trainer.state.output)\n",
    "#     )\n",
    "#     wandb.log({\"train loss\": trainer.state.output})\n",
    "\n",
    "\n",
    "# @trainer.on(Events.EPOCH_COMPLETED)\n",
    "# def log_training_results(trainer):\n",
    "#     train_evaluator.run(train_dataloader)\n",
    "#     metrics = train_evaluator.state.metrics\n",
    "#     logger.info(\n",
    "#         \"Training Results - Epoch: {} MSE: {:.7f} MAE: {:.7f}\".format(\n",
    "#             trainer.state.epoch, metrics[\"mse\"], metrics[\"mae\"]\n",
    "#         )\n",
    "#     )\n",
    "\n",
    "\n",
    "# @trainer.on(Events.EPOCH_COMPLETED)\n",
    "# def log_validation_results(trainer):\n",
    "#     validation_evaluator.run(test_dataloader)\n",
    "#     metrics = validation_evaluator.state.metrics\n",
    "#     logger.info(\n",
    "#         \"Validation Results - Epoch: {} MSE: {:.7f} MAE: {:.7f}\".format(\n",
    "#             trainer.state.epoch, metrics[\"mse\"], metrics[\"mae\"]\n",
    "#         )\n",
    "#     )\n",
    "#     lr_scheduler.step(metrics[\"mae\"])\n",
    "#     wandb.log({\"val mae\": metrics['mae']})\n",
    "#     wandb.log({\"val mse\": metrics['mse']})\n",
    "\n",
    "\n",
    "# trainer.run(train_dataloader, max_epochs=100)\n",
    "# # We need to close the logger with we are done\n",
    "# tb_logger.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:epigcn] *",
   "language": "python",
   "name": "conda-env-epigcn-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
