{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import logging\n",
    "import h5py\n",
    "import gzip\n",
    "import json\n",
    "import os\n",
    "import tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interact_manual\n",
    "\n",
    "logging.basicConfig()\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model definition for loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class BaselineLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super(BaselineLSTM, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            batch_first=True,\n",
    "            num_layers=num_layers,\n",
    "        )\n",
    "        self.linear = nn.Linear(hidden_size * num_layers, input_size)\n",
    "\n",
    "    def forward(self, lstm_input):\n",
    "        \"\"\"\n",
    "        :lstm_input: (b, 232, 232, 10)\n",
    "        \"\"\"\n",
    "        #         print(lstm_input.size())\n",
    "        b, n_countries, n_countries, seq_len = lstm_input.size()\n",
    "        lstm_input = lstm_input.permute(0, 3, 1, 2)\n",
    "        _, (hn, __) = self.lstm(lstm_input.contiguous().view(b, seq_len, -1))\n",
    "        hn = hn.permute(1, 0, 2)\n",
    "        return self.linear(hn.reshape(b, -1)).view(b, n_countries, n_countries, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BaselineLSTM(input_size=232*232, hidden_size=1024, num_layers=10)\n",
    "\n",
    "model.load_state_dict(\n",
    "    torch.load(\n",
    "        \"model_checkpoints/country_level-lr=0.001,input_size=53824,hidden_size=1024,num_layers=10_epi_64_mae=0.02303406.pth\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup data and get predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "scaler = joblib.load('data/EpiGCN/standard_scaler.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(\"data/EpiGCN/train_test.hdf5\", \"r\") as f:\n",
    "    x_train = f[\"x_train\"].value[..., 0]\n",
    "    x_test = f[\"x_test\"].value[..., 0]\n",
    "    y_train = f[\"y_train\"].value[..., 0]\n",
    "    y_test = f[\"y_test\"].value[..., 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X train shape:  (312, 232, 232, 10)\n",
      "X test shape:  (34, 232, 232, 10)\n",
      "Y train shape:  (312, 232, 232, 1)\n",
      "Y test shape:  (34, 232, 232, 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"X train shape: \", x_train.shape)\n",
    "print(\"X test shape: \", x_test.shape)\n",
    "print(\"Y train shape: \", y_train.shape)\n",
    "print(\"Y test shape: \", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "use_cuda = torch.cuda.is_available()\n",
    "# device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "device = torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scaled_output(y):\n",
    "    shape = y.shape\n",
    "    y = y.reshape(-1)\n",
    "    mean = scaler.mean_[0]\n",
    "    scale = scaler.scale_[0]\n",
    "    return (scale * y + mean).reshape(*shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "\n",
    "train_dataset = TensorDataset(torch.FloatTensor(x_train), torch.FloatTensor(y_train))\n",
    "test_dataset = TensorDataset(torch.FloatTensor(x_test), torch.FloatTensor(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params = {\"batch_size\": 12, \"shuffle\": True, \"num_workers\": 4}\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=12, shuffle=True, num_workers=0)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=12, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions, truth = [], []\n",
    "with torch.no_grad():\n",
    "    for batch_x, batch_y in test_dataloader:\n",
    "        logits = model(batch_x.to(device))\n",
    "        predictions.append(get_scaled_output(logits.cpu().numpy()))\n",
    "        truth.append(get_scaled_output(batch_y.cpu().numpy()))\n",
    "predictions = np.concatenate(predictions)\n",
    "truth = np.concatenate(truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(34, 232, 232, 1)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "truth.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "\n",
    "def animate(i):\n",
    "#     fig, axes = plt.subplots(1, 2, figsize=(25, 8))\n",
    "#     axes[0].set_title(\"Test: Truth\")\n",
    "#     axes[1].set_title(\"Test: Predictions\")\n",
    "    t = truth[i, :, :, 0]\n",
    "    p = predictions[i, :, :, 0]\n",
    "    vmin = min(t.min(), p.min())\n",
    "    vmax = max(t.max(), p.max())\n",
    "    sns.heatmap(t, vmin=vmin, vmax=vmax, cmap=\"Blues\", ax=axes[0])\n",
    "    sns.heatmap(p, vmin=vmin, vmax=vmax, cmap=\"Blues\", ax=axes[1])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Animator implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import os\n",
    "from tqdm import tnrange\n",
    "import imageio\n",
    "\n",
    "\n",
    "class Animator:\n",
    "    def __init__(self, truth, predictions, dir_name=\"visualizations\"):\n",
    "        self.time_dim = truth.shape[0]\n",
    "        self.truth = truth\n",
    "        self.predictions = predictions\n",
    "        self.dir_name = dir_name\n",
    "        self.vmax = max(truth.max(), predictions.max())\n",
    "        self.vmin = min(truth.min(), predictions.min())\n",
    "\n",
    "        if not os.path.exists(dir_name):\n",
    "            os.makedirs(dir_name)\n",
    "\n",
    "    def list_pngs(self):\n",
    "        return list(\n",
    "            map(\n",
    "                lambda file: os.path.join(self.dir_name, file),\n",
    "                filter(lambda file: file.endswith(\".png\"), os.listdir(self.dir_name)),\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def remove_pngs(self):\n",
    "        for file in self.list_pngs():\n",
    "            os.remove(file)\n",
    "\n",
    "    def animate_img(self, i, name):\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(25, 8))\n",
    "        axes[0].set_title(\"{}: Truth\".format(name))\n",
    "        axes[1].set_title(\"{}: Predictions\".format(name))\n",
    "        t = self.truth[i, :, :, 0]\n",
    "        p = self.predictions[i, :, :, 0]\n",
    "        sns.heatmap(t, vmin=self.vmin, vmax=self.vmax, cmap=\"Blues\", ax=axes[0])\n",
    "        sns.heatmap(p, vmin=self.vmin, vmax=self.vmax, cmap=\"Blues\", ax=axes[1])\n",
    "        plt.savefig(\"{}/{}.png\".format(self.dir_name, i))\n",
    "\n",
    "    def create_gif(self, name):\n",
    "        images = []\n",
    "        for file in self.list_pngs():\n",
    "            images.append(imageio.imread(file))\n",
    "        imageio.mimsave(os.path.join(self.dir_name, \"{}.gif\".format(name)), images)\n",
    "\n",
    "    def animate(self, name):\n",
    "        self.remove_pngs()\n",
    "\n",
    "        for i in tnrange(self.time_dim):\n",
    "            self.animate_img(i, name)\n",
    "\n",
    "        self.create_gif(name)\n",
    "        self.remove_pngs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "animator = Animator(truth, predictions)\n",
    "animator.animate(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=12, shuffle=False, num_workers=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predictions, train_truth = [], []\n",
    "with torch.no_grad():\n",
    "    for batch_x, batch_y in train_dataloader:\n",
    "        logits = model(batch_x.to(device))\n",
    "        train_predictions.append(get_scaled_output(logits.cpu()))\n",
    "        train_truth.append(get_scaled_output(batch_y.cpu()))\n",
    "train_predictions = np.concatenate(train_predictions)\n",
    "train_truth = np.concatenate(train_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "animator = Animator(train_truth, train_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "animator.animate(\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize after rearranging the grid based on graph clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "clustered_graph = nx.read_gpickle('data/EpiGCN/hcs_components.gpkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should now create a mapping from older index to newer index so that we can rearrange the grid - the older index is the natural `country_id` order and the newer index is an incrementing integer (starting from 0), this ensures that components in the same graph are clustered together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reordered_mapping_of_nodes(clustered_graph):\n",
    "    \"\"\"\n",
    "    takes in a networkx graph with each subgraph being it's own connected component\n",
    "    \"\"\"\n",
    "    ix = 0\n",
    "    mapping = {}\n",
    "    for component in nx.connected_components(clustered_graph):\n",
    "        for node in component:\n",
    "            mapping[node] = ix\n",
    "            ix += 1\n",
    "    return mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustered_mapping = get_reordered_mapping_of_nodes(clustered_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reorder original grids based on the new mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reordered_grid(grid, clustered_mapping):\n",
    "    new_grid = np.zeros_like(grid)\n",
    "    for old_ix, new_ix in clustered_mapping.items():\n",
    "        new_grid[:, new_ix] = grid[:, old_ix] # first axis is time\n",
    "    return new_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "reordered_truth = get_reordered_grid(truth, clustered_mapping)\n",
    "reordered_predictions = get_reordered_grid(predictions, clustered_mapping)\n",
    "reordered_train_truth = get_reordered_grid(train_truth, clustered_mapping)\n",
    "reordered_train_predictions = get_reordered_grid(train_predictions, clustered_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original shape: (34, 232, 232, 1), new shape: (34, 232, 232, 1)\n",
      "original shape: (34, 232, 232, 1), new shape: (34, 232, 232, 1)\n",
      "original shape: (312, 232, 232, 1), new shape: (312, 232, 232, 1)\n",
      "original shape: (312, 232, 232, 1), new shape: (312, 232, 232, 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"original shape: {}, new shape: {}\".format(truth.shape, reordered_truth.shape))\n",
    "print(\"original shape: {}, new shape: {}\".format(predictions.shape, reordered_predictions.shape))\n",
    "print(\"original shape: {}, new shape: {}\".format(train_truth.shape, reordered_train_truth.shape))\n",
    "print(\"original shape: {}, new shape: {}\".format(train_predictions.shape, reordered_train_predictions.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "animator = Animator(reordered_truth, reordered_predictions)\n",
    "animator.animate('reordered test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:epigcn] *",
   "language": "python",
   "name": "conda-env-epigcn-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
